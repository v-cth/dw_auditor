# ============================================================================
# Data Warehouse Audit Configuration
# ============================================================================
# This YAML file configures the table auditor behavior, including database
# connections, sampling, security settings, and output formats.

# Audit Metadata (Optional)
# ----------------------------------------------------------------------------
# Descriptive information about this audit configuration
# These fields will be displayed in the Metadata tab of HTML reports
version: 1
project: "Bike Availability Data Quality"
description: "Auditing availability fact table and station dimension for data quality issues"
last_modified: "2025-10-24"

# Database Connection
# ----------------------------------------------------------------------------
database:
  # Database backend: 'bigquery' or 'snowflake'
  backend: "bigquery"

  # Connection parameters (backend-specific)
  connection_params:
    # For BigQuery:
    # project_id: Your billing project (where queries are executed)
    # schema: Default dataset for queries (can be overridden per-table)
    # source_project_id: (Optional) Source project for cross-project queries
    #                    Use this when querying public datasets or external projects
    #                    Jobs will run in project_id, data read from source_project_id

    # Example: Querying public datasets
    project_id: "portfolio-402"  # Your project (you have bigquery.jobs.create here)
    schema: "dbt_prod"  # Default dataset for queries
    #source_project_id: "bigquery-public-data"  # Public dataset project (read-only)
    #schema: "crypto_bitcoin"  # Dataset to query, very heavy !!!!!!!

    #credentials_path: "/path/to/service-account-key.json"
    # Or use credentials_json for JSON string/dict

    # For Snowflake (comment out BigQuery params and uncomment these):
    # account: "my-account"
    # user: "my-user"
    # password: "my-password"
    # database: "my_database"
    # schema: "my_schema"  # Default schema for queries
    # warehouse: "my_warehouse"
    # role: "my_role"  # optional

# Tables to Audit
# ----------------------------------------------------------------------------
# Option 1: Explicit list of tables (recommended for small number of tables)
tables:
  - name: fct_avaibility_part
  - name: dim_station
    primary_key: station_code  # Optional: specify primary key column(s)
  # - name: dtm_station_distance
  # - name: transactions
  #   primary_key: transaction_id
  #   schema: dbt_staging       # Optional: override default schema for this table
  #   query: "SELECT * FROM transactions LIMIT 3000"


  # - name: blocks
  #   primary_key: block_id
  #   query: "SELECT * FROM blocks LIMIT 2000"

  # Example: Auditing tables from multiple datasets in one run
  # - name: raw_customers
  #   schema: raw_data          # From raw_data dataset
  # - name: stg_customers
  #   schema: staging           # From staging dataset
  # - name: dim_customers
  #   schema: dbt_prod          # From production dataset

# Option 2: Auto-discover tables and apply filters
# ----------------------------------------------------------------------------
# Uncomment to enable auto-discovery (will discover all tables if tables list is empty)
# table_filters:
#   # Enable auto-discovery of all tables in schema
#   auto_discover: true
#
#   # Exclude tables matching these patterns (applied first)
#   exclude_patterns:
#     - "stg_*"          # Exclude staging tables with stg_ prefix
#
#   # Include only tables matching these patterns (applied second, optional)
#   include_patterns:
#     - "fct_*"          # Only fact tables



# Column-Level Check Configuration Matrix
# ----------------------------------------------------------------------------
# Configure which checks to run on which columns
column_checks:
  # Global defaults per data type
  defaults:
    string:
      # Check for leading/trailing characters or strings
      # Can be: true (default whitespace), false (disabled),
      #         string " " (single char), or list [" ", "_dim", "_tmp"] (multiple patterns)
      trailing_characters: true
      # Check for specific ending characters or patterns
      # Can be: true (default punctuation .,;:!?), false (disabled),
      #         string ".," (check these chars), or list [".", "_tmp"] (multiple patterns)
      ending_characters: false
      case_duplicates: true
      # Regex pattern validation/detection
      # NO DEFAULT - must be explicitly configured with a pattern
      # Can be: false (disabled, default),
      #         string "[^a-zA-Z]" (pattern with contains mode),
      #         or dict with pattern/mode/description:
      #         regex_patterns:
      #           pattern: "^[A-Z]{2}\\d{4}$"
      #           mode: "match"  # or "contains"
      #           description: "Station code format"
      regex_patterns: false
      numeric_strings: true
      uniqueness: false  # Set true to flag duplicate values (e.g., for ID columns)
    datetime:
      timestamp_patterns: true
      date_outliers: true
      future_dates: true
      uniqueness: false  # Set true to flag duplicate timestamps
    numeric:
      uniqueness: false  # Set true to flag duplicate numeric values (e.g., for IDs)

  # Per-table, per-column overrides and numeric range validation
  tables:
  #   dim_station:
  #     # Uniqueness check - verify column values are unique
  #     station_code:
  #       uniqueness: true  # Flag duplicate station codes
  #
  #     # Check for specific trailing patterns
  #     table_name:
  #       trailing_characters: ["_dim", "_tmp"]  # Check if values end with _dim or _tmp
  #
  #     # Check for specific ending patterns
  #     description:
  #       ending_characters: [".", "!"]  # Check if descriptions end with . or !
  #
  #     # Validate station code format
  #     station_code:
  #       regex_patterns:
  #         pattern: "^[A-Z]{2}\\d{4}$"
  #         mode: "match"
  #         description: "Station code must be 2 letters + 4 digits"
  #
  #     # Detect special characters in city names (but expect some)
  #     city:
  #       regex_patterns: false  # Disable - French accents are expected
  #     # Skip case duplicates and regex checks for station_name (expected variation & French accents)
  #     station_name:
  #       case_duplicates: false
  #       regex_patterns: false  # Disable - French accents are expected
  #
  #     # Numeric range validation (supports 4 boundary operators)
  #     capacity:
  #       greater_than_or_equal: 1    # Inclusive: capacity >= 1
  #       less_than_or_equal: 100     # Inclusive: capacity <= 100
  #       uniqueness: false           # Capacity can have duplicates
  #     latitude:
  #       greater_than_or_equal: -90  # Inclusive: latitude >= -90
  #       less_than_or_equal: 90      # Inclusive: latitude <= 90
  #     longitude:
  #       greater_than: -180          # Exclusive: longitude > -180
  #       less_than: 180              # Exclusive: longitude < 180
  #
    fct_avaibility_part:         
      available_docks:
        greater_than_or_equal: 1
      bikes_available:
        greater_than: 0
        uniqueness: true


    dim_station:
      station_code:
        uniqueness: true
      capacity:
        less_than: 20

# Column Insights Configuration
# ----------------------------------------------------------------------------
# Configure data profiling insights (separate from quality checks)
column_insights:
  # Global defaults per data type
  # defaults:
  #   string:
  #     # Show top N most frequent values with counts and percentages
  #     top_values: 10
  #     # String length statistics
  #     min_length: true
  #     max_length: true
  #     avg_length: true

  #   numeric:
  #     # Basic statistics
  #     min: true
  #     max: true
  #     mean: true
  #     median: true
  #     std: false  # Standard deviation (can be noisy)
  #     # Percentile breakdowns
  #     quantiles: [0.25, 0.5, 0.75]
  #     # Show top N most frequent values
  #     top_values: 5

  #   datetime:
  #     # Date range information
  #     min_date: true
  #     max_date: true
  #     date_range_days: true
  #     # Timezone info (for timezone-aware columns)
  #     most_common_timezones: 1
  #     # Most common dates with counts
  #     most_common_dates: 5
  #     # Most common days of week
  #     most_common_days: 7
  #     # Most common hours (0-23)
  #     most_common_hours: 5

  # Per-table, per-column overrides
  # tables:
  #   dim_station:
  #     # Get detailed distribution for capacity column
  #     capacity:
  #       top_values: 5
  #       quantiles: [0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]
  #     # Minimal insights for station_code (primary key)
  #     station_code:
  #       top_values: 0  # Disable - primary key

  #   fct_avaibility:
  #     # Detailed date analysis for timestamp columns
  #     last_reported:
  #       most_common_dates: 3

# Relationship Detection
# ----------------------------------------------------------------------------
# Automatically detect relationships between tables based on column names,
# data types, and value overlaps (uses already-loaded Polars DataFrames)
relationship_detection:
  # Enable/disable relationship detection (only runs when auditing 2+ tables)
  enabled: true

  # Minimum confidence score to detect a relationship (0.0 to 1.0)
  # Higher values = fewer but more confident relationships
  # Confidence calculation: 40% name similarity + 20% type compatibility + 40% value overlap
  confidence_threshold: 0.7

  # Minimum confidence score to display in reports (0.0 to 1.0)
  # Allows detecting high-confidence relationships but showing moderate ones too
  min_confidence_display: 0.5

  # Note: Uses already-loaded DataFrames from audit (respects sampling.sample_size)
  # No additional database queries are performed

# Sampling Configuration
# ----------------------------------------------------------------------------
sampling:
  # Number of rows to sample (samples when table has more rows than this)
  sample_size: 10000

  # Use database-native sampling (faster, more secure)
  # If true, sampling happens in the database before loading data
  sample_in_db: true

  # Sampling strategy (global default)
  # Options: random, recent, top, systematic
  method: "random"

  # Column to use for non-random sampling (optional, required for recent/top/systematic)
  # For 'recent': sorts descending, takes last N rows
  # For 'top': sorts ascending, takes first N rows
  # For 'systematic': uses modulo filter (e.g., WHERE id % stride = 0)
  key_column: null

  # Per-table sampling overrides
  # tables:
  #   fct_avaibility:
  #     method: "recent"
  #     key_column: "collected_at"  # Sample most recent data
    # dim_station:
    #   method: "random"  # Explicit random for this table

# Security Settings
# ----------------------------------------------------------------------------
security:
  # Automatically mask columns containing PII
  mask_pii: true
  
  # Additional keywords to identify PII columns (beyond defaults)
  # Default keywords: ssn, email, phone, address, credit_card, password, etc.
  custom_pii_keywords:
    - "employee_id"
    - "customer_number"
    - "internal_code"
    - "confidential"

# Detection Thresholds (percentages)
# ----------------------------------------------------------------------------
thresholds:
  # Percentage of numeric values to flag a string column as "should be numeric"
  # Example: If 85% of values in a string column are numbers, flag it
  numeric_string_pct: 80

  # Percentage threshold for constant hour detection
  # Example: If 92% of timestamps have the same hour, flag as date-only
  constant_hour_pct: 90

  # Percentage threshold for midnight timestamps
  # Example: If 96% of timestamps are at midnight, suggest DATE type
  midnight_pct: 95

  # Date outlier detection thresholds
  # Minimum reasonable year (dates before this are flagged)
  min_year: 2020

  # Maximum reasonable year (dates after this are flagged)
  max_year: 2100

  # Minimum percentage to report as outlier (default: 0.0 = report all)
  # Only report if at least this % of rows have outliers
  outlier_threshold_pct: 0.0

# Output Configuration
# ----------------------------------------------------------------------------
output:
  # Directory where audit results will be saved
  directory: "audit_results"

  # Export formats: html, json, csv, parquet
  formats:
    - html      # Beautiful interactive report
    - csv       # For Excel/analysis
    - json      # For APIs/automation

  # Prefix for output filenames
  # Final format: {prefix}_{table_name}_{timestamp}.{extension}
  file_prefix: "audit"

  # Automatically open HTML reports in browser after generation
  auto_open_html: true

  # Number formatting for HTML reports
  number_format:
    # Thousand separator
    # Options: "," (American/English), " " (European/International), "_" (underscore)
    thousand_separator: ","

    # Decimal places for floating point numbers
    # Options: 0 (integer), 1, 2, etc.
    decimal_places: 1

# Column Filters (Optional)
# ----------------------------------------------------------------------------
# Use these to limit which columns are audited
filters:
  # If specified, ONLY audit these columns (leave empty to audit all)
  include_columns: []
    # - "customer_name"
    # - "email"
    # - "created_at"
  
  # If specified, SKIP these columns
  exclude_columns: []
    # - "internal_metadata"
    # - "system_field"
    # - "deprecated_column"


# ============================================================================
# Example Configurations for Different Scenarios
# ============================================================================

# --- Example 1: Minimal Configuration ---
# database:
#   connection_string: "postgresql://user:pass@localhost/db"
# tables:
#   - users
#   - orders
# output:
#   directory: "audit_results"
#   formats: [html]

# --- Example 2: High-Security Configuration ---
# database:
#   connection_string: "postgresql://user:pass@localhost/db"
# tables:
#   - sensitive_data
# security:
#   mask_pii: true
#   custom_pii_keywords:
#     - "proprietary"
#     - "confidential"
#     - "restricted"
# sampling:
#   sample_in_db: true
#   sample_size: 10000
# output:
#   directory: "secure_audits"
#   formats: [html, json]

# --- Example 3: Production Data Warehouse ---
# database:
#   connection_string: "postgresql://readonly:pass@prod-warehouse/analytics"
#   schema: "public"
# tables:
#   - fact_sales
#   - fact_orders
#   - dim_customers
#   - dim_products
#   - dim_time
# sampling:
#   sample_size: 500000
#   sample_in_db: true
# security:
#   mask_pii: true
# output:
#   directory: "/shared/audit_reports"
#   formats: [html, csv, parquet]
#   file_prefix: "prod_audit"

# --- Example 4: Quick Development Audit ---
# database:
#   connection_string: "sqlite:///local.db"
# tables:
#   - test_table
# checks:
#   trailing_spaces: true
#   case_duplicates: true
#   special_characters: false
#   numeric_strings: false
#   timestamp_patterns: false
# sampling:
#   sample_size: 10000
# output:
#   directory: "quick_audit"
#   formats: [html]

# --- Example 5: Specific Columns Only ---
# database:
#   connection_string: "postgresql://user:pass@localhost/db"
# tables:
#   - customer_data
# filters:
#   include_columns:
#     - "email"
#     - "phone"
#     - "address"
#     - "name"
# output:
#   directory: "column_audit"
#   formats: [csv]

# --- Example 6: Exclude System Columns ---
# database:
#   connection_string: "postgresql://user:pass@localhost/db"
# tables:
#   - application_logs
# filters:
#   exclude_columns:
#     - "id"
#     - "created_at"
#     - "updated_at"
#     - "deleted_at"
#     - "version"
# output:
#   formats: [html]

# --- Example 7: Custom Thresholds ---
# database:
#   connection_string: "postgresql://user:pass@localhost/db"
# tables:
#   - legacy_data
# thresholds:
#   numeric_string_pct: 95  # Very strict
#   constant_hour_pct: 99   # Very strict
#   midnight_pct: 99        # Very strict
# output:
#   formats: [html, csv]

# --- Example 8: Multiple Tables with Custom Queries ---
# database:
#   connection_string: "postgresql://user:pass@localhost/analytics"
#   schema: "public"
# tables:
#   - name: recent_sales
#     query: "SELECT * FROM sales WHERE sale_date >= CURRENT_DATE - INTERVAL '7 days'"
#   - name: active_customers
#     query: "SELECT * FROM customers WHERE last_login > CURRENT_DATE - INTERVAL '30 days'"
#   - name: high_value_orders
#     query: "SELECT * FROM orders WHERE total_amount > 1000"
# security:
#   mask_pii: true
# output:
#   directory: "filtered_audits"
#   formats: [html, csv, json]
#   file_prefix: "filtered"