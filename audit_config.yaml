# ============================================================================
# Data Warehouse Audit Configuration
# ============================================================================
# This YAML file configures the table auditor behavior, including database
# connections, sampling, security settings, and output formats.

# Database Connection
# ----------------------------------------------------------------------------
database:
  # Database backend: 'bigquery' or 'snowflake'
  backend: "bigquery"

  # Connection parameters (backend-specific)
  connection_params:
    # For BigQuery:
    # project_id: Your billing project (where queries are executed)
    # dataset_id: Dataset to query
    # source_project_id: (Optional) Source project for cross-project queries
    #                    Use this when querying public datasets or external projects
    #                    Jobs will run in project_id, data read from source_project_id

    # Example: Querying public datasets
    project_id: "portfolio-402"  # Your project (you have bigquery.jobs.create here)
    #source_project_id: "bigquery-public-data"  # Public dataset project (read-only)
    #dataset_id: "crypto_bitcoin"  # Dataset to query, very heavy !!!!!!!

    # Example: Querying your own datasets (no source_project_id needed)
    # project_id: "portfolio-402"
    dataset_id: "dbt_prod"

    #credentials_path: "/path/to/service-account-key.json"
    # Or use credentials_json for JSON string/dict

    # For Snowflake (comment out BigQuery params and uncomment these):
    # account: "my-account"
    # user: "my-user"
    # password: "my-password"
    # database: "my_database"
    # warehouse: "my_warehouse"
    # role: "my_role"  # optional

  # Optional: Schema name (overrides connection default)
  # For BigQuery, this would be the dataset
  # For Snowflake, this would be the schema
  schema: null

# Tables to Audit
# ----------------------------------------------------------------------------
# Option 1: Explicit list of tables (recommended for small number of tables)
tables:
  - name: fct_avaibility_part
  - name: dim_station
    primary_key: station_code  # Optional: specify primary key column(s)
  - name: dtm_station_distance
  # - name: transactions
  #   primary_key: transaction_id 
  #   query: "SELECT * FROM transactions LIMIT 3000"


  # - name: blocks
  #   primary_key: block_id 
  #   query: "SELECT * FROM blocks LIMIT 2000"

# Option 2: Auto-discover tables and apply filters
# ----------------------------------------------------------------------------
# Uncomment to enable auto-discovery (will discover all tables if tables list is empty)
# table_filters:
#   # Enable auto-discovery of all tables in schema
#   auto_discover: true
#
#   # Exclude tables matching these patterns (applied first)
#   exclude_patterns:
#     - "stg_*"          # Exclude staging tables with stg_ prefix
#
#   # Include only tables matching these patterns (applied second, optional)
#   include_patterns:
#     - "fct_*"          # Only fact tables



# Column-Level Check Configuration Matrix
# ----------------------------------------------------------------------------
# Configure which checks to run on which columns
column_checks:
  # Global defaults per data type
  defaults:
    string:
      trailing_spaces: true
      case_duplicates: true
      special_chars: true
      numeric_strings: true
    datetime:
      timestamp_patterns: true
      date_outliers: true
      future_dates: true

  # Per-table, per-column overrides and numeric range validation
  # tables:
  #   dim_station:
  #     # Skip special chars check for city column (we expect French accents)
  #     city:
  #       special_chars: false
  #     # Skip case duplicates and special chars for station_name (expected variation & French accents)
  #     station_name:
  #       case_duplicates: false
  #       special_chars: false  # Skip - French accents are expected
  #
  #     # Numeric range validation (supports 6 boundary operators)
  #     capacity:
  #       min: 1                      # Inclusive: capacity >= 1
  #       max: 100                    # Inclusive: capacity <= 100
  #     latitude:
  #       greater_than_or_equal: -90  # Inclusive: latitude >= -90
  #       less_than_or_equal: 90      # Inclusive: latitude <= 90
  #     longitude:
  #       greater_than: -180          # Exclusive: longitude > -180
  #       less_than: 180              # Exclusive: longitude < 180
  #
  #   fct_avaibility_part:
  #     available_bikes:
  #       min: 0                      # No negative bikes
  #     available_docks:
  #       greater_than_or_equal: 0    # Same as min: 0

# Column Insights Configuration
# ----------------------------------------------------------------------------
# Configure data profiling insights (separate from quality checks)
column_insights:
  # Global defaults per data type
  defaults:
    string:
      # Show top N most frequent values with counts and percentages
      top_values: 10
      # String length statistics
      min_length: true
      max_length: true
      avg_length: true

    numeric:
      # Basic statistics
      min: true
      max: true
      mean: true
      median: true
      std: false  # Standard deviation (can be noisy)
      # Percentile breakdowns
      quantiles: [0.25, 0.5, 0.75]
      # Show top N most frequent values
      top_values: 5

    datetime:
      # Date range information
      min_date: true
      max_date: true
      date_range_days: true
      # Timezone info (for timezone-aware columns)
      most_common_timezones: 1
      # Most common dates with counts
      most_common_dates: 5
      # Most common days of week
      most_common_days: 7
      # Most common hours (0-23)
      most_common_hours: 10

  # Per-table, per-column overrides
  # tables:
  #   dim_station:
  #     # Get detailed distribution for capacity column
  #     capacity:
  #       top_values: 5
  #       quantiles: [0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]
  #     # Minimal insights for station_code (primary key)
  #     station_code:
  #       top_values: 0  # Disable - primary key

  #   fct_avaibility:
  #     # Detailed date analysis for timestamp columns
  #     last_reported:
  #       most_common_dates: 3

# Sampling Configuration
# ----------------------------------------------------------------------------
sampling:
  # Number of rows to sample if table exceeds threshold
  sample_size: 10000

  # Row count threshold to trigger sampling
  sample_threshold: 10000

  # Use database-native sampling (faster, more secure)
  # If true, sampling happens in the database before loading data
  sample_in_db: true

  # Sampling strategy (global default)
  # Options: random, recent, top, systematic
  method: "random"

  # Column to use for non-random sampling (optional, required for recent/top/systematic)
  # For 'recent': sorts descending, takes last N rows
  # For 'top': sorts ascending, takes first N rows
  # For 'systematic': uses modulo filter (e.g., WHERE id % stride = 0)
  key_column: null

  # Per-table sampling overrides
  # tables:
  #   fct_avaibility:
  #     method: "recent"
  #     key_column: "collected_at"  # Sample most recent data
    # dim_station:
    #   method: "random"  # Explicit random for this table

# Security Settings
# ----------------------------------------------------------------------------
security:
  # Automatically mask columns containing PII
  mask_pii: true
  
  # Additional keywords to identify PII columns (beyond defaults)
  # Default keywords: ssn, email, phone, address, credit_card, password, etc.
  custom_pii_keywords:
    - "employee_id"
    - "customer_number"
    - "internal_code"
    - "confidential"

# Quality Checks Configuration
# ----------------------------------------------------------------------------
# Enable/disable specific checks
checks:
  # Check for leading/trailing whitespace
  trailing_spaces: true

  # Check for values that differ only in case (e.g., "JOHN" vs "john")
  case_duplicates: true

  # Check for special characters (emojis, accents, etc.)
  special_characters: true

  # Check for string columns containing only numbers
  numeric_strings: true

  # Check for timestamps that are effectively dates
  timestamp_patterns: true

  # Check for date/timestamp outliers (very old or future dates)
  date_outliers: true

  # Check for dates/datetimes that are in the future (relative to current time)
  future_dates: true

  # Custom regex pattern for special characters
  # Default: [^a-zA-Z0-9\s\.,\-_@]
  special_chars_pattern: "[^a-zA-Z0-9\\s\\.,\\-_@]"

# Detection Thresholds (percentages)
# ----------------------------------------------------------------------------
thresholds:
  # Percentage of numeric values to flag a string column as "should be numeric"
  # Example: If 85% of values in a string column are numbers, flag it
  numeric_string_pct: 80

  # Percentage threshold for constant hour detection
  # Example: If 92% of timestamps have the same hour, flag as date-only
  constant_hour_pct: 90

  # Percentage threshold for midnight timestamps
  # Example: If 96% of timestamps are at midnight, suggest DATE type
  midnight_pct: 95

  # Date outlier detection thresholds
  # Minimum reasonable year (dates before this are flagged)
  min_year: 2020

  # Maximum reasonable year (dates after this are flagged)
  max_year: 2100

  # Minimum percentage to report as outlier (default: 0.0 = report all)
  # Only report if at least this % of rows have outliers
  outlier_threshold_pct: 0.0

# Output Configuration
# ----------------------------------------------------------------------------
output:
  # Directory where audit results will be saved
  directory: "audit_results"

  # Export formats: html, json, csv, parquet
  formats:
    - html      # Beautiful interactive report
    - csv       # For Excel/analysis
    - json      # For APIs/automation

  # Prefix for output filenames
  # Final format: {prefix}_{table_name}_{timestamp}.{extension}
  file_prefix: "audit"

  # Automatically open HTML reports in browser after generation
  auto_open_html: true

  # Number formatting for HTML reports
  number_format:
    # Thousand separator
    # Options: "," (American/English), " " (European/International), "_" (underscore)
    thousand_separator: ","

    # Decimal places for floating point numbers
    # Options: 0 (integer), 1, 2, etc.
    decimal_places: 1

# Column Filters (Optional)
# ----------------------------------------------------------------------------
# Use these to limit which columns are audited
filters:
  # If specified, ONLY audit these columns (leave empty to audit all)
  include_columns: []
    # - "customer_name"
    # - "email"
    # - "created_at"
  
  # If specified, SKIP these columns
  exclude_columns: []
    # - "internal_metadata"
    # - "system_field"
    # - "deprecated_column"


# ============================================================================
# Example Configurations for Different Scenarios
# ============================================================================

# --- Example 1: Minimal Configuration ---
# database:
#   connection_string: "postgresql://user:pass@localhost/db"
# tables:
#   - users
#   - orders
# output:
#   directory: "audit_results"
#   formats: [html]

# --- Example 2: High-Security Configuration ---
# database:
#   connection_string: "postgresql://user:pass@localhost/db"
# tables:
#   - sensitive_data
# security:
#   mask_pii: true
#   custom_pii_keywords:
#     - "proprietary"
#     - "confidential"
#     - "restricted"
# sampling:
#   sample_in_db: true
#   sample_size: 10000
# output:
#   directory: "secure_audits"
#   formats: [html, json]

# --- Example 3: Production Data Warehouse ---
# database:
#   connection_string: "postgresql://readonly:pass@prod-warehouse/analytics"
#   schema: "public"
# tables:
#   - fact_sales
#   - fact_orders
#   - dim_customers
#   - dim_products
#   - dim_time
# sampling:
#   sample_size: 500000
#   sample_threshold: 5000000
#   sample_in_db: true
# security:
#   mask_pii: true
# output:
#   directory: "/shared/audit_reports"
#   formats: [html, csv, parquet]
#   file_prefix: "prod_audit"

# --- Example 4: Quick Development Audit ---
# database:
#   connection_string: "sqlite:///local.db"
# tables:
#   - test_table
# checks:
#   trailing_spaces: true
#   case_duplicates: true
#   special_characters: false
#   numeric_strings: false
#   timestamp_patterns: false
# sampling:
#   sample_threshold: 10000
# output:
#   directory: "quick_audit"
#   formats: [html]

# --- Example 5: Specific Columns Only ---
# database:
#   connection_string: "postgresql://user:pass@localhost/db"
# tables:
#   - customer_data
# filters:
#   include_columns:
#     - "email"
#     - "phone"
#     - "address"
#     - "name"
# output:
#   directory: "column_audit"
#   formats: [csv]

# --- Example 6: Exclude System Columns ---
# database:
#   connection_string: "postgresql://user:pass@localhost/db"
# tables:
#   - application_logs
# filters:
#   exclude_columns:
#     - "id"
#     - "created_at"
#     - "updated_at"
#     - "deleted_at"
#     - "version"
# output:
#   formats: [html]

# --- Example 7: Custom Thresholds ---
# database:
#   connection_string: "postgresql://user:pass@localhost/db"
# tables:
#   - legacy_data
# thresholds:
#   numeric_string_pct: 95  # Very strict
#   constant_hour_pct: 99   # Very strict
#   midnight_pct: 99        # Very strict
# output:
#   formats: [html, csv]

# --- Example 8: Multiple Tables with Custom Queries ---
# database:
#   connection_string: "postgresql://user:pass@localhost/analytics"
#   schema: "public"
# tables:
#   - name: recent_sales
#     query: "SELECT * FROM sales WHERE sale_date >= CURRENT_DATE - INTERVAL '7 days'"
#   - name: active_customers
#     query: "SELECT * FROM customers WHERE last_login > CURRENT_DATE - INTERVAL '30 days'"
#   - name: high_value_orders
#     query: "SELECT * FROM orders WHERE total_amount > 1000"
# security:
#   mask_pii: true
# output:
#   directory: "filtered_audits"
#   formats: [html, csv, json]
#   file_prefix: "filtered"